{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "974fd60a",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74a88ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio\n",
    "\n",
    "import torchaudio\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor, Trainer, TrainingArguments, Wav2Vec2ForSequenceClassification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba4098e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae84d4",
   "metadata": {},
   "source": [
    "Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87fddb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "for dirname, _ , filenames in os.walk('./dataset'):\n",
    "    for filename in filenames:\n",
    "        paths.append(os.path.join(dirname, filename))\n",
    "        label = filename.split('_')[-1]\n",
    "        label = label.split('.')[0]\n",
    "        labels.append(label.lower())\n",
    "        \n",
    "    if len(paths) == 2800:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c2968",
   "metadata": {},
   "source": [
    "Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a79eec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['audio_path'] = paths\n",
    "df['label'] = labels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974bf0f",
   "metadata": {},
   "source": [
    "Create custom dataset class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to integers\n",
    "labels_map = {label: idx for idx, label in enumerate(df['label'].unique())}\n",
    "inverse_label_map = {idx:label for label, idx in labels_map.items()}\n",
    "df['label'] = df['label'].map(labels_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad015290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "class SpeechEmotionDataset(Dataset):\n",
    "    def __init__(self, df, processor, max_length=32000):\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.df.iloc[idx]['audio_path']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "\n",
    "        # Load the audio file\n",
    "        speech, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "        # Pad or truncate\n",
    "        if len(speech) > self.max_length:\n",
    "            speech = speech[:self.max_length]\n",
    "        else:\n",
    "            speech = np.pad(speech, (0, self.max_length - len(speech)), 'constant')\n",
    "\n",
    "        # Preprocess with the processor\n",
    "        inputs = self.processor(\n",
    "            speech,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_values = inputs.input_values.squeeze()  \n",
    "        return {\n",
    "            'input_values': input_values,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdac6fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01cfa592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initial processor and model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained('facebook/wav2vec2-base', num_labels = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb9e51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpeechEmotionDataset(train_df, processor)\n",
    "test_dataset = SpeechEmotionDataset(test_df, processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9de335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aa3ba0",
   "metadata": {},
   "source": [
    "Set Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6624857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',\n",
    "    eval_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 3,\n",
    "    weight_decay = 0.01,\n",
    "    report_to = [],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af367b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for computing the metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids                    \n",
    "    preds  = np.argmax(pred.predictions, axis=1)  \n",
    "\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"weighted\"  \n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"accuracy\":  accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\":    recall,\n",
    "        \"f1\":        f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eebaff00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='420' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [420/420 1:48:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.493792</td>\n",
       "      <td>0.996429</td>\n",
       "      <td>0.996509</td>\n",
       "      <td>0.996429</td>\n",
       "      <td>0.996426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.217652</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.169286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=420, training_loss=0.5904856727236794, metrics={'train_runtime': 6539.8616, 'train_samples_per_second': 1.028, 'train_steps_per_second': 0.064, 'total_flos': 1.2201848064e+17, 'train_loss': 0.5904856727236794, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial the trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c502b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16928617656230927, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 68.6735, 'eval_samples_per_second': 8.155, 'eval_steps_per_second': 0.51, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec48c6f",
   "metadata": {},
   "source": [
    "Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8910c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "idx = random.randrange(0, len(test_dataset))\n",
    "input_values = test_dataset[idx]['input_values'].unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_values)\n",
    "\n",
    "logits = outputs.logits\n",
    "\n",
    "predicted_class = logits.argmax(dim = -1).item()\n",
    "print('predicted_class: ', predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
